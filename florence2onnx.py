import os
import time
from typing import List

import numpy as np
from PIL import Image
import onnxruntime as ort
from transformers import AutoProcessor


class Florence2OnnxModel:
    def __init__(
        self,
        providers: List[str] = None,
        warmup_iterations: int = 10,
        verbose: bool = False,
    ):
        # Set the working directory to the current ONNX model directory.
        onnx_dir: str = os.path.dirname(os.path.abspath(__file__))
        os.chdir(onnx_dir)

        processor_dir: str = os.path.join(onnx_dir, "processor_files")

        if providers is None:
            providers = ["CPUExecutionProvider"]

        # Create custom session options.
        # If verbose is False, we set a higher log severity level (3) to suppress warnings.
        # When verbose is True, we lower the threshold to see detailed logs.
        session_options = ort.SessionOptions()
        session_options.log_severity_level = 1 if verbose else 3
        session_options.graph_optimization_level = (
            ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        )

        # Optionally, you could enforce memory pattern optimizations (enabled by default)
        session_options.enable_mem_pattern = True
        session_options.enable_cpu_mem_arena = True

        # Initialize ONNX sessions with the custom session options.
        self.vision_encoder = ort.InferenceSession(
            os.path.join(onnx_dir, "weight_files/vision_encoder_q4f16.onnx"),
            providers=providers,
            sess_options=session_options,
        )
        self.text_embed = ort.InferenceSession(
            os.path.join(onnx_dir, "weight_files/embed_tokens_q4f16.onnx"),
            providers=providers,
            sess_options=session_options,
        )
        self.encoder = ort.InferenceSession(
            os.path.join(onnx_dir, "weight_files/encoder_model_q4f16.onnx"),
            providers=providers,
            sess_options=session_options,
        )
        self.decoder_prefill = ort.InferenceSession(
            os.path.join(onnx_dir, "weight_files/decoder_model_q4f16.onnx"),
            providers=providers,
            sess_options=session_options,
        )
        self.decoder_decode = ort.InferenceSession(
            os.path.join(onnx_dir, "weight_files/decoder_model_merged_q4.onnx"),
            providers=providers,
            sess_options=session_options,
        )

        self.processor = AutoProcessor.from_pretrained(
            processor_dir, trust_remote_code=True
        )

        self._warmup(iterations=warmup_iterations)

    def _warmup(self, iterations: int = 10) -> None:
        dummy_image = Image.new("RGB", (384, 384))
        dummy_prompt = "<MORE_DETAILED_CAPTION>"
        dummy_inputs = self.processor(
            text=dummy_prompt, images=dummy_image, return_tensors="np"
        )

        for _ in range(iterations):
            _ = self.vision_encoder.run(
                None, {"pixel_values": dummy_inputs["pixel_values"]}
            )
            _ = self.text_embed.run(None, {"input_ids": dummy_inputs["input_ids"]})
            _ = self.encoder.run(
                None,
                {
                    "inputs_embeds": np.zeros((1, 10, 768), dtype=np.float32),
                    "attention_mask": np.zeros((1, 10), dtype=np.int64),
                },
            )

    def generate_caption(
        self,
        image_path: str,
        prompt: str = "<MORE_DETAILED_CAPTION>",
        max_new_tokens: int = 1024,
    ) -> (str, float):

        image = Image.open(image_path)
        inputs = self.processor(
            text=prompt, images=image, return_tensors="np", do_resize=True
        )

        start_time = time.time()

        image_features = self.vision_encoder.run(
            None, {"pixel_values": inputs["pixel_values"]}
        )[0]

        inputs_embeds = self.text_embed.run(None, {"input_ids": inputs["input_ids"]})[0]

        batch_size, image_token_length = image_features.shape[:-1]
        image_attention_mask = np.ones((batch_size, image_token_length), dtype=np.int64)
        task_prefix_embeds = inputs_embeds
        task_prefix_attention_mask = np.ones(
            (batch_size, task_prefix_embeds.shape[1]), dtype=np.int64
        )

        if task_prefix_attention_mask.ndim == 3:
            task_prefix_attention_mask = task_prefix_attention_mask[:, 0]

        inputs_embeds = np.concatenate([image_features, task_prefix_embeds], axis=1)
        attention_mask = np.concatenate(
            [image_attention_mask, task_prefix_attention_mask], axis=1
        )

        encoder_hidden_states = self.encoder.run(
            None, {"inputs_embeds": inputs_embeds, "attention_mask": attention_mask}
        )[0]

        decoder_outs = self.decoder_prefill.run(
            None,
            {
                "inputs_embeds": inputs_embeds[:, -1:],
                "encoder_hidden_states": encoder_hidden_states,
                "encoder_attention_mask": attention_mask,
            },
        )
        encoder_kv = decoder_outs[1:]

        generated_tokens = []
        while len(generated_tokens) < max_new_tokens:
            logits = decoder_outs[0]
            decoder_kv = decoder_outs[1:]

            next_token_logits = logits[:, -1, :]
            next_token = int(np.argmax(next_token_logits, axis=-1)[0])
            generated_tokens.append(next_token)

            # Stop if the end-of-sequence token (assumed token id 2) is generated.
            if next_token == 2:
                break

            next_input_embeds = self.text_embed.run(
                None, {"input_ids": np.array([[next_token]], dtype=np.int64)}
            )[0]

            decoder_outs = self.decoder_decode.run(
                None,
                {
                    "use_cache_branch": np.array([True], dtype=np.bool_),
                    "inputs_embeds": next_input_embeds,
                    "encoder_hidden_states": encoder_hidden_states,
                    "encoder_attention_mask": attention_mask,
                    "past_key_values.0.decoder.key": decoder_kv[0],
                    "past_key_values.0.decoder.value": decoder_kv[1],
                    "past_key_values.0.encoder.key": encoder_kv[2],
                    "past_key_values.0.encoder.value": encoder_kv[3],
                    "past_key_values.1.decoder.key": decoder_kv[4],
                    "past_key_values.1.decoder.value": decoder_kv[5],
                    "past_key_values.1.encoder.key": encoder_kv[6],
                    "past_key_values.1.encoder.value": encoder_kv[7],
                    "past_key_values.2.decoder.key": decoder_kv[8],
                    "past_key_values.2.decoder.value": decoder_kv[9],
                    "past_key_values.2.encoder.key": encoder_kv[10],
                    "past_key_values.2.encoder.value": encoder_kv[11],
                    "past_key_values.3.decoder.key": decoder_kv[12],
                    "past_key_values.3.decoder.value": decoder_kv[13],
                    "past_key_values.3.encoder.key": encoder_kv[14],
                    "past_key_values.3.encoder.value": encoder_kv[15],
                    "past_key_values.4.decoder.key": decoder_kv[16],
                    "past_key_values.4.decoder.value": decoder_kv[17],
                    "past_key_values.4.encoder.key": encoder_kv[18],
                    "past_key_values.4.encoder.value": encoder_kv[19],
                    "past_key_values.5.decoder.key": decoder_kv[20],
                    "past_key_values.5.decoder.value": decoder_kv[21],
                    "past_key_values.5.encoder.key": encoder_kv[22],
                    "past_key_values.5.encoder.value": encoder_kv[23],
                },
            )

        end_time = time.time()
        total_time = end_time - start_time

        generated_text = self.processor.batch_decode(
            [generated_tokens], skip_special_tokens=False
        )[0]

        parsed_answer = self.processor.post_process_generation(
            generated_text, task=prompt, image_size=(image.width, image.height)
        )
        return parsed_answer, total_time

    def infer_from_image(
        self,
        image_path: str,
        prompt: str = "<MORE_DETAILED_CAPTION>",
        max_new_tokens: int = 1024,
    ) -> None:
        parsed_answer, inference_time = self.generate_caption(
            image_path, prompt, max_new_tokens
        )
        print(f"Inference Time: {inference_time:.4f} seconds")
        print("Answer:", parsed_answer)


if __name__ == "__main__":
    # Set verbose=True to see detailed logs; otherwise warnings are suppressed.
    model = Florence2OnnxModel(
        providers=["CUDAExecutionProvider"], warmup_iterations=10, verbose=False
    )
    model.infer_from_image(
        "/home/ubuntu/git/star-ai/notebooks/inpaint/test.jpg",
        prompt="<MORE_DETAILED_CAPTION>",
        max_new_tokens=1024,
    )
